# Gradient Descent

Goal : To minmize the value of w and b for a particular linear regression instance. Also useful in other areas as well. For example in training deep learning models as well. 

### We can use gradient descent to minmize any function not only the cost function. 

## Steps: 
>> 1. Start with some w and b , usually both 0. 
>> 2. Keep changing w, b to reduced J(w,b).
>> 3. Keep doing it until there's a minimum J(w,b).

Remember, sometimes there's more than one minimum for a particular cost function. 

### Key terms : 
>> 1. Alpha : Learning Rate 
>> 2. We need to do simulateneous update , while calculating the terms of w and b while talking about a cost function. 
>> 3. Learning rate needs to be carefully chosen. 

